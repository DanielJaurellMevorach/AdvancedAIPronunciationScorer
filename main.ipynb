{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d43805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The project is not supposed to be ran from a jupyter notebook,\n",
    "# but this might help you understand the code better.\n",
    "\n",
    "# CSV data read caused issues, until exporting\n",
    "export PYTHONUTF8=1 # if using git bash\n",
    "set PYTHONUTF8=1 # if using cmd\n",
    "\n",
    "# venv\n",
    "source venv/bin/activate  # For Unix or MacOS\n",
    "\n",
    "# Install required packages\n",
    "pip install -r requirements.txt\n",
    "pip install pydub\n",
    "\n",
    "you might also have to install ffmpeg\n",
    "\n",
    "# Run the main script\n",
    "python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c3f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IMPORTS ---\n",
    "# Standard library imports\n",
    "import json\n",
    "import base64\n",
    "import random\n",
    "import string\n",
    "\n",
    "import os\n",
    "\n",
    "import abc\n",
    "\n",
    "# Third-party imports\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchaudio.transforms import Resample\n",
    "from flask import Flask, render_template, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from transformers import pipeline\n",
    "from dtwalign import dtw_from_distance_matrix\n",
    "import epitran\n",
    "import eng_to_ipa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3dd17",
   "metadata": {},
   "source": [
    "## Advanced AI Pronunciation Scorer – Application Overview\n",
    "\n",
    "This Flask‑based web application provides pronunciation scoring for multiple languages (English and French) using advanced speech recognition and phonetic analysis techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "The application follows a modular design with clear separation of concerns through abstract interfaces and concrete implementations.\n",
    "\n",
    "#### Core Interfaces\n",
    "\n",
    "- **IASRModel** (Abstract Speech Recognition Interface)  \n",
    "  - **Purpose:** Defines the contract for Automatic Speech Recognition models  \n",
    "  - **Key Methods:**  \n",
    "    - `getTranscript()`: Returns the transcribed text  \n",
    "    - `getWordLocations()`: Returns word timing information  \n",
    "    - `processAudio()`: Processes audio input  \n",
    "\n",
    "- **ITextToPhonemModel** (Abstract Phoneme Conversion Interface)  \n",
    "  - **Purpose:** Defines the contract for text‑to‑phoneme conversion  \n",
    "  - **Key Method:**  \n",
    "    - `convertToPhonem()`: Converts text to IPA phonetic representation  \n",
    "\n",
    "---\n",
    "\n",
    "#### Concrete Implementations\n",
    "\n",
    "- **WhisperASRModel** (implements `IASRModel`)  \n",
    "  - **Purpose:** Wraps OpenAI’s Whisper model for speech recognition  \n",
    "  - **Features:**  \n",
    "    - Uses Hugging Face `transformers` pipeline  \n",
    "    - Handles audio preprocessing & transcription  \n",
    "    - Supports multiple languages through model selection  \n",
    "\n",
    "- **Phoneme Converters**  \n",
    "  - **EngPhonemConverter** (implements `ITextToPhonemModel`)  \n",
    "    - Uses the `eng‑to‑ipa` library for English  \n",
    "  - **EpitranPhonemConverter** (implements `ITextToPhonemModel`)  \n",
    "    - Uses the `epitran` library for French  \n",
    "\n",
    "- **PronunciationTrainer** \n",
    "  - (Core Processing Class)  \n",
    "  - **Purpose:** Orchestrates the entire pronunciation analysis pipeline  \n",
    "  - **Dependencies:**  \n",
    "    - Accepts an `IASRModel` and an `ITextToPhonemModel` in its constructor  \n",
    "    - Uses Dynamic Time Warping (DTW) for word alignment  \n",
    "    - Implements phoneme‑based accuracy scoring  \n",
    "\n",
    "- **TextDataset**  \n",
    "  - **Purpose:** Manages practice sentences from CSV files  \n",
    "  - **Functionality:** Provides random sentence sampling for different languages  \n",
    "\n",
    "---\n",
    "\n",
    "### Request Flow Analysis\n",
    "\n",
    "#### 1. Flask Route: `/score_pronunciation` (POST)\n",
    "\n",
    "When a user submits audio for scoring, the request follows this flow:\n",
    "\n",
    "1. **Flask Route Handler (`score_pronunciation()`)**  \n",
    "   - Receives JSON with:  \n",
    "     - `text` (reference sentence)  \n",
    "     - `base64_audio`  \n",
    "     - `language`  \n",
    "   - Validates language support  \n",
    "   - Decodes Base64 audio to raw bytes  \n",
    "\n",
    "2. **Audio Preprocessing**  \n",
    "   - Saves audio to a temporary file  \n",
    "   - Loads audio using `soundfile`  \n",
    "   - Converts to mono, normalizes amplitude  \n",
    "   - Resamples to 16 kHz via `torchaudio.Resample`  \n",
    "\n",
    "3. **Trainer Selection and Processing**  \n",
    "   - Selects the appropriate `PronunciationTrainer` based on language  \n",
    "   - Calls `trainer.process_audio_for_text(signal, real_text)`  \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. PronunciationTrainer Processing Pipeline\n",
    "\n",
    "**`process_audio_for_text()` Method Flow:**\n",
    "\n",
    "1. **Audio Preprocessing** (`_preprocess_audio()`)  \n",
    "   - Normalize to zero mean  \n",
    "   - Scale to the \\[-1, 1\\] range  \n",
    "\n",
    "2. **Speech Recognition** (`_get_transcript_and_locations()`)  \n",
    "   - Uses injected `WhisperASRModel`  \n",
    "   - Calls `asr_model.processAudio(audio)`  \n",
    "   - Retrieves transcript via `asr_model.getTranscript()`  \n",
    "\n",
    "3. **Word Alignment** (`get_best_mapped_words()`)  \n",
    "   - Split both reference and ASR transcript into word lists  \n",
    "   - Build edit‑distance cost matrix between words  \n",
    "   - Apply DTW (`dtwalign` library) for optimal alignment  \n",
    "   - Return mapped word pairs  \n",
    "\n",
    "4. **Pronunciation Analysis** (`get_pronunciation_accuracy()`)  \n",
    "   - Convert each word to IPA via the phoneme converter  \n",
    "   - Compute Levenshtein distance between phoneme sequences  \n",
    "   - Calculate per‑word and overall accuracy scores  \n",
    "\n",
    "5. **Letter‑Level Analysis** (`get_which_letters_were_correct()`)  \n",
    "   - Perform character‑level alignment  \n",
    "   - Generate binary correctness markers for frontend highlighting  \n",
    "\n",
    "6. **Result Assembly**  \n",
    "   - Build JSON with:  \n",
    "     - `real_transcript`  \n",
    "     - `recording_transcript`  \n",
    "     - `matched_transcripts`  \n",
    "     - `pronunciation_accuracy`  \n",
    "     - `pair_accuracy_category`  \n",
    "     - `is_letter_correct_all_words`  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Algorithms and Techniques\n",
    "\n",
    "- **Dynamic Time Warping (DTW)**  \n",
    "  - **Purpose:** Aligns estimated words with reference words optimally  \n",
    "  - **Implementation:** Uses `dtwalign` with a custom distance matrix  \n",
    "\n",
    "- **Edit Distance (Levenshtein)**  \n",
    "  - **Usage:**  \n",
    "    - Word‑level alignment  \n",
    "    - Phoneme‑level pronunciation accuracy  \n",
    "    - Character‑level correctness detection  \n",
    "\n",
    "- **Context‑Aware Scoring**  \n",
    "  - **Problem Solved:** Prevents a few mispronounced words from unduly penalizing the whole score  \n",
    "  - **Logic:**  \n",
    "    - If > 60 % of words are correctly transcribed, apply a small “boost” to overall accuracy  \n",
    "  - **Benefit:** More realistic, encouraging feedback for learners  \n",
    "\n",
    "---\n",
    "\n",
    "### Class Relationships & Dependency Injection\n",
    "\n",
    "- **Dependency Injection Pattern**  \n",
    "  - `PronunciationTrainer` depends on abstract interfaces, not concrete implementations  \n",
    "  - Language‑specific converters and ASR models can be swapped easily  \n",
    "  - Facilitates unit testing and future extension  \n",
    "\n",
    "---\n",
    "\n",
    "### Data Flow Summary\n",
    "\n",
    "1. **Input:** Audio file + reference text + language  \n",
    "2. **ASR Processing:** Audio → Whisper → Transcribed text  \n",
    "3. **Alignment:** Reference words ↔ Transcribed words via DTW  \n",
    "4. **Scoring:** Word pairs → IPA phonemes → Edit distance → Accuracy scores  \n",
    "5. **Visualization:** Character alignment → Highlighting data  \n",
    "6. **Output:** JSON payload with accuracy metrics and highlighting info  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4564219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL INTERFACES ---\n",
    "class IASRModel(metaclass=abc.ABCMeta):\n",
    "    \"\"\"Abstract Base Class for Automatic Speech Recognition models.\"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def getTranscript(self) -> str:\n",
    "        raise NotImplementedError\n",
    "    @abc.abstractmethod\n",
    "    def getWordLocations(self) -> list:\n",
    "        raise NotImplementedError\n",
    "    @abc.abstractmethod\n",
    "    def processAudio(self, audio):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class ITextToPhonemModel(metaclass=abc.ABCMeta):\n",
    "    \"\"\"Abstract Base Class for Text-to-Phoneme models.\"\"\"\n",
    "    @abc.abstractmethod\n",
    "    def convertToPhonem(self, str) -> str:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "# --- PHONEME CONVERTERS ---\n",
    "class EpitranPhonemConverter(ITextToPhonemModel):\n",
    "    \"\"\"Converts French text to phonemes using the 'epitran' library.\"\"\"\n",
    "    def __init__(self, epitran_model) -> None:\n",
    "        self.epitran_model = epitran_model\n",
    "    def convertToPhonem(self, sentence: str) -> str:\n",
    "        return self.epitran_model.transliterate(sentence)\n",
    "\n",
    "class EngPhonemConverter(ITextToPhonemModel):\n",
    "    \"\"\"Converts English text to phonemes using the 'eng-to-ipa' library.\"\"\"\n",
    "    def convertToPhonem(self, sentence: str) -> str:\n",
    "        phonem_representation = eng_to_ipa.convert(sentence)\n",
    "        return phonem_representation.replace('*', '')\n",
    "    \n",
    "# --- WHISPER ASR MODEL ---\n",
    "class WhisperASRModel(IASRModel):\n",
    "    \"\"\"Wrapper for Whisper ASR model from Hugging Face transformers.\"\"\"\n",
    "    def __init__(self, model_name='openai/whisper-base'):\n",
    "        self.asr = pipeline(\n",
    "            'automatic-speech-recognition',\n",
    "            model=model_name,\n",
    "            device=torch.device('cpu'),\n",
    "            model_kwargs={\"attn_implementation\": \"eager\"}\n",
    "        )\n",
    "        self._transcript = ''\n",
    "        self._word_locations = []\n",
    "        self.sample_rate = 16000\n",
    "\n",
    "    def processAudio(self, audio: np.ndarray):\n",
    "        \"\"\"Processes audio and extracts transcript (fallback without timestamps due to tensor bug).\"\"\"\n",
    "        try:\n",
    "            # Just get transcript without timestamps to avoid the tensor shape bug\n",
    "            result = self.asr(audio)\n",
    "            self._transcript = result['text']\n",
    "            self._word_locations = []  # Empty for now due to Whisper bug\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Whisper processing: {e}\")\n",
    "            self._transcript = \"\"\n",
    "            self._word_locations = []\n",
    "\n",
    "    def getTranscript(self) -> str:\n",
    "        return self._transcript\n",
    "\n",
    "    def getWordLocations(self) -> list:\n",
    "        return self._word_locations\n",
    "    \n",
    "# --- PRONUNCIATION TRAINER ---\n",
    "class PronunciationTrainer:\n",
    "    \"\"\"Main class that orchestrates the pronunciation scoring process.\"\"\"\n",
    "    def __init__(self, asr_model: IASRModel, phonem_converter: ITextToPhonemModel, sampling_rate: int = 16000):\n",
    "        self.asr_model = asr_model\n",
    "        self.ipa_converter = phonem_converter\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.categories_thresholds = np.array([80, 50, 0])\n",
    "\n",
    "    def _preprocess_audio(self, audio: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Normalizes audio to zero mean and scales to [-1, 1].\"\"\"\n",
    "        if audio.any():\n",
    "            audio = audio - np.mean(audio)\n",
    "            audio = audio / np.max(np.abs(audio))\n",
    "        return audio\n",
    "\n",
    "    def _get_transcript_and_locations(self, audio: np.ndarray):\n",
    "        \"\"\"Processes audio to get transcript using ASR model.\"\"\"\n",
    "        self.asr_model.processAudio(audio)\n",
    "        transcript = self.asr_model.getTranscript()\n",
    "        word_locations = self.asr_model.getWordLocations()\n",
    "        return transcript, word_locations\n",
    "\n",
    "    def _remove_punctuation(self, text: str) -> str:\n",
    "        \"\"\"Removes all punctuation from text.\"\"\"\n",
    "        return ''.join(char for char in text if char not in string.punctuation)\n",
    "        \n",
    "    def get_pronunciation_accuracy(self, real_and_transcribed_words: list) -> tuple:\n",
    "        \"\"\"Calculates pronunciation accuracy using IPA phoneme comparison.\"\"\"\n",
    "        total_distance = 0\n",
    "        total_phonemes = 0\n",
    "        word_accuracies = []\n",
    "        \n",
    "        # Track how many words were actually transcribed vs missing\n",
    "        transcribed_words = 0\n",
    "        total_words = 0\n",
    "\n",
    "        for real_word, trans_word in real_and_transcribed_words:\n",
    "            real_word_clean = self._remove_punctuation(real_word).lower()\n",
    "            trans_word_clean = self._remove_punctuation(trans_word).lower() if trans_word != '-' else ''\n",
    "\n",
    "            total_words += 1\n",
    "\n",
    "            # Skip empty real words (punctuation only)\n",
    "            if not real_word_clean:\n",
    "                word_accuracies.append(100)  # Punctuation gets full credit\n",
    "                continue\n",
    "\n",
    "            real_ipa = self.ipa_converter.convertToPhonem(real_word_clean)\n",
    "            \n",
    "            if not real_ipa: \n",
    "                word_accuracies.append(0)\n",
    "                continue\n",
    "\n",
    "            # If no transcribed word, score based on context\n",
    "            if not trans_word_clean:\n",
    "                # Give partial credit if most other words were transcribed correctly\n",
    "                word_accuracies.append(0)  # Will be adjusted later\n",
    "                total_distance += len(real_ipa)\n",
    "                total_phonemes += len(real_ipa)\n",
    "            else:\n",
    "                transcribed_words += 1\n",
    "                trans_ipa = self.ipa_converter.convertToPhonem(trans_word_clean)\n",
    "                \n",
    "                if not trans_ipa:\n",
    "                    distance = len(real_ipa)\n",
    "                else:\n",
    "                    distance = edit_distance(real_ipa, trans_ipa)\n",
    "                \n",
    "                num_phonemes = len(real_ipa)\n",
    "                total_distance += distance\n",
    "                total_phonemes += num_phonemes\n",
    "                \n",
    "                # Calculate word accuracy\n",
    "                accuracy = ((num_phonemes - distance) / num_phonemes) * 100 if num_phonemes > 0 else 0\n",
    "                word_accuracies.append(max(0, accuracy))\n",
    "\n",
    "        # Calculate overall accuracy with context-aware scoring\n",
    "        if total_phonemes > 0:\n",
    "            overall_accuracy = ((total_phonemes - total_distance) / total_phonemes) * 100\n",
    "            overall_accuracy = max(0, overall_accuracy)\n",
    "            \n",
    "            # If user got most words right, don't penalize too heavily for a few mumbled words\n",
    "            transcription_rate = transcribed_words / max(total_words, 1)\n",
    "            if transcription_rate > 0.6:  # If more than 60% of words were transcribed\n",
    "                # Boost the score to reflect that most pronunciation was good\n",
    "                overall_accuracy = min(100, overall_accuracy * (1 + transcription_rate * 0.3))\n",
    "                \n",
    "                # Also boost individual word scores for missing words in good contexts\n",
    "                for i, (real_word, trans_word) in enumerate(real_and_transcribed_words):\n",
    "                    if trans_word == '-' and word_accuracies[i] == 0:\n",
    "                        # Give some partial credit for missing words when overall performance is good\n",
    "                        word_accuracies[i] = min(30, overall_accuracy * 0.3)\n",
    "            \n",
    "        else:\n",
    "            overall_accuracy = 0\n",
    "\n",
    "        return (np.round(max(0, overall_accuracy)), word_accuracies)\n",
    "\n",
    "    def get_words_pronunciation_category(self, accuracies: list) -> list:\n",
    "        \"\"\"Converts accuracy scores to categories (1=Good, 2=Medium, 3=Poor).\"\"\"\n",
    "        categories = []\n",
    "        for accuracy in accuracies:\n",
    "            category_index = np.argmin(np.abs(self.categories_thresholds - accuracy))\n",
    "            categories.append(category_index + 1)\n",
    "        return categories\n",
    "\n",
    "    def process_audio_for_text(self, recorded_audio: np.ndarray, real_text: str) -> dict:\n",
    "        \"\"\"Main method that processes audio and returns pronunciation analysis.\"\"\"\n",
    "        # 1. Preprocess audio and get transcript\n",
    "        processed_audio = self._preprocess_audio(recorded_audio)\n",
    "        recording_transcript, _ = self._get_transcript_and_locations(processed_audio)\n",
    "        \n",
    "        # 2. Align words using DTW\n",
    "        words_real = real_text.split()\n",
    "        words_estimated = recording_transcript.split()\n",
    "        mapped_words, _ = get_best_mapped_words(words_estimated, words_real)\n",
    "        \n",
    "        real_and_transcribed_words = list(zip(words_real, mapped_words))\n",
    "\n",
    "        # 3. Calculate pronunciation accuracy using IPA phoneme comparison\n",
    "        pronunciation_accuracy, word_accuracies = self.get_pronunciation_accuracy(real_and_transcribed_words)\n",
    "        pronunciation_categories = self.get_words_pronunciation_category(word_accuracies)\n",
    "        \n",
    "        # 4. Letter-level correctness for highlighting\n",
    "        is_letter_correct_all_words = ''\n",
    "        for idx, real_word in enumerate(words_real):\n",
    "            transcribed_word = mapped_words[idx]\n",
    "            is_letter_correct = get_which_letters_were_correct(real_word, transcribed_word)\n",
    "            is_letter_correct_all_words += ''.join(map(str, is_letter_correct)) + ' '\n",
    "            \n",
    "        # 5. Return results\n",
    "        result = {\n",
    "            'real_transcript': real_text,\n",
    "            'recording_transcript': recording_transcript,\n",
    "            'matched_transcripts': ' '.join(mapped_words),\n",
    "            'pronunciation_accuracy': str(int(pronunciation_accuracy)),\n",
    "            'pair_accuracy_category': ' '.join(map(str, pronunciation_categories)),\n",
    "            'is_letter_correct_all_words': is_letter_correct_all_words.strip()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "# --- DATA LOADING ---\n",
    "class TextDataset:\n",
    "    \"\"\"Simple class to hold sentences from a CSV file.\"\"\"\n",
    "    def __init__(self, filepath):\n",
    "        self.table_dataframe = pd.read_csv(filepath, delimiter=';')\n",
    "        self.number_of_samples = len(self.table_dataframe)\n",
    "\n",
    "    def get_random_sample(self):\n",
    "        \"\"\"Returns a random sentence from the dataset.\"\"\"\n",
    "        idx = random.randint(0, self.number_of_samples - 1)\n",
    "        return self.table_dataframe['sentence'].iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ab67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UTILITY FUNCTIONS ---\n",
    "def edit_distance(seq1: str, seq2: str) -> int:\n",
    "    \"\"\"Calculates the Levenshtein (edit) distance between two sequences.\"\"\"\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros((size_x, size_y), dtype=int)\n",
    "    for x in range(size_x):\n",
    "        matrix[x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix[0, y] = y\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            cost = 0 if seq1[x - 1] == seq2[y - 1] else 1\n",
    "            matrix[x, y] = min(\n",
    "                matrix[x - 1, y] + 1,        # Deletion\n",
    "                matrix[x - 1, y - 1] + cost, # Substitution\n",
    "                matrix[x, y - 1] + 1,        # Insertion\n",
    "            )\n",
    "    return matrix[size_x - 1, size_y - 1]\n",
    "\n",
    "def generate_random_string(str_length: int = 20) -> str:\n",
    "    \"\"\"Generates a random string for unique temporary filenames.\"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join((random.choice(letters) for _ in range(str_length)))\n",
    "\n",
    "# --- WORD MATCHING LOGIC ---\n",
    "def get_word_distance_matrix(words_estimated: list, words_real: list) -> np.ndarray:\n",
    "    \"\"\"Creates a cost matrix for word alignment using edit distance with phonetic similarity.\"\"\"\n",
    "    number_of_real_words = len(words_real)\n",
    "    number_of_estimated_words = len(words_estimated)\n",
    "    word_distance_matrix = np.zeros((number_of_estimated_words, number_of_real_words))\n",
    "\n",
    "    for idx_estimated, est_word in enumerate(words_estimated):\n",
    "        for idx_real, real_word in enumerate(words_real):\n",
    "            # Basic edit distance\n",
    "            distance = edit_distance(est_word.lower(), real_word.lower())\n",
    "            max_len = max(len(est_word), len(real_word), 1)\n",
    "            normalized_distance = distance / max_len\n",
    "            \n",
    "            # Bonus for words that start with the same letter(s)\n",
    "            if est_word.lower().startswith(real_word.lower()[:2]) or real_word.lower().startswith(est_word.lower()[:2]):\n",
    "                normalized_distance *= 0.8\n",
    "            \n",
    "            # Bonus for similar length words\n",
    "            length_diff = abs(len(est_word) - len(real_word)) / max_len\n",
    "            if length_diff < 0.3:  # Similar length\n",
    "                normalized_distance *= 0.9\n",
    "            \n",
    "            word_distance_matrix[idx_estimated, idx_real] = normalized_distance\n",
    "\n",
    "    return word_distance_matrix\n",
    "\n",
    "def get_best_mapped_words(words_estimated: list, words_real: list) -> tuple:\n",
    "    \"\"\"Uses DTW to align estimated words with real words.\"\"\"\n",
    "    if not words_estimated or not words_real:\n",
    "        return (['-'] * len(words_real), [-1] * len(words_real))\n",
    "\n",
    "    try:\n",
    "        # Clean punctuation\n",
    "        words_real_clean = [word.strip(string.punctuation).lower() for word in words_real]\n",
    "        words_estimated_clean = [word.strip(string.punctuation).lower() for word in words_estimated]\n",
    "        \n",
    "        word_distance_matrix = get_word_distance_matrix(words_estimated_clean, words_real_clean)\n",
    "        \n",
    "        # Use DTW for alignment\n",
    "        alignment = dtw_from_distance_matrix(word_distance_matrix.T)\n",
    "        \n",
    "        # Get the alignment path\n",
    "        path_query = alignment.path[:, 0]  # Real word indices\n",
    "        path_reference = alignment.path[:, 1]  # Estimated word indices\n",
    "        \n",
    "        # Initialize result arrays\n",
    "        mapped_words = ['-'] * len(words_real)\n",
    "        mapped_words_indices = [-1] * len(words_real)\n",
    "        \n",
    "        # Create mapping from the DTW path, but allow multiple mappings\n",
    "        used_estimated = set()\n",
    "        for real_idx, est_idx in zip(path_query, path_reference):\n",
    "            if 0 <= real_idx < len(words_real) and 0 <= est_idx < len(words_estimated):\n",
    "                # Only map if this estimated word hasn't been used or if it's a better match\n",
    "                if est_idx not in used_estimated or mapped_words[real_idx] == '-':\n",
    "                    mapped_words[real_idx] = words_estimated[est_idx]\n",
    "                    mapped_words_indices[real_idx] = est_idx\n",
    "                    used_estimated.add(est_idx)\n",
    "\n",
    "        # Post-process: Find good matches that DTW might have missed\n",
    "        for real_idx, real_word in enumerate(words_real_clean):\n",
    "            if mapped_words[real_idx] == '-':  # No mapping found by DTW\n",
    "                best_match_idx = -1\n",
    "                best_similarity = 0\n",
    "                \n",
    "                for est_idx, est_word in enumerate(words_estimated_clean):\n",
    "                    if est_idx in used_estimated:\n",
    "                        continue\n",
    "                        \n",
    "                    # Calculate similarity\n",
    "                    distance = edit_distance(real_word, est_word)\n",
    "                    max_len = max(len(real_word), len(est_word), 1)\n",
    "                    similarity = 1 - (distance / max_len)\n",
    "                    \n",
    "                    # Lower threshold for good matches\n",
    "                    if similarity > best_similarity and similarity > 0.6:\n",
    "                        best_similarity = similarity\n",
    "                        best_match_idx = est_idx\n",
    "                \n",
    "                if best_match_idx != -1:\n",
    "                    mapped_words[real_idx] = words_estimated[best_match_idx]\n",
    "                    mapped_words_indices[real_idx] = best_match_idx\n",
    "                    used_estimated.add(best_match_idx)\n",
    "\n",
    "        print(f\"Real words: {words_real}\")\n",
    "        print(f\"Estimated words: {words_estimated}\")\n",
    "        print(f\"DTW alignment path (real->est): {list(zip(path_query, path_reference))}\")\n",
    "        print(f\"Mapped words: {mapped_words}\")\n",
    "        \n",
    "        return (mapped_words, mapped_words_indices)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in DTW alignment: {e}\")\n",
    "        # Enhanced fallback mapping\n",
    "        mapped_words = ['-'] * len(words_real)\n",
    "        mapped_words_indices = [-1] * len(words_real)\n",
    "        \n",
    "        used_estimated_indices = set()\n",
    "        \n",
    "        # First pass: find exact or very close matches\n",
    "        for real_idx, real_word in enumerate(words_real_clean):\n",
    "            best_match_idx = -1\n",
    "            best_similarity = 0\n",
    "            \n",
    "            for est_idx, est_word in enumerate(words_estimated_clean):\n",
    "                if est_idx in used_estimated_indices:\n",
    "                    continue\n",
    "                    \n",
    "                distance = edit_distance(real_word, est_word)\n",
    "                max_len = max(len(real_word), len(est_word), 1)\n",
    "                similarity = 1 - (distance / max_len)\n",
    "                \n",
    "                if similarity > best_similarity and similarity > 0.6:  # Higher threshold for fallback\n",
    "                    best_similarity = similarity\n",
    "                    best_match_idx = est_idx\n",
    "            \n",
    "            if best_match_idx != -1:\n",
    "                mapped_words[real_idx] = words_estimated[best_match_idx]\n",
    "                mapped_words_indices[real_idx] = best_match_idx\n",
    "                used_estimated_indices.add(best_match_idx)\n",
    "        \n",
    "        print(f\"Fallback mapping: {mapped_words}\")\n",
    "        return (mapped_words, mapped_words_indices)\n",
    "\n",
    "def get_which_letters_were_correct(real_word: str, transcribed_word: str) -> list:\n",
    "    \"\"\"Compares words letter by letter for front-end highlighting using better alignment.\"\"\"\n",
    "    is_letter_correct = []\n",
    "    real_word_lower = real_word.lower()\n",
    "    transcribed_word_lower = transcribed_word.lower() if transcribed_word != '-' else ''\n",
    "    \n",
    "    if transcribed_word == '-' or not transcribed_word_lower:\n",
    "        # No transcription - mark all as incorrect except punctuation\n",
    "        for char in real_word:\n",
    "            if char in string.punctuation:\n",
    "                is_letter_correct.append(1)\n",
    "            else:\n",
    "                is_letter_correct.append(0)\n",
    "        return is_letter_correct\n",
    "    \n",
    "    # Use dynamic programming to find best character alignment\n",
    "    real_len = len(real_word_lower)\n",
    "    trans_len = len(transcribed_word_lower)\n",
    "    \n",
    "    # DP table for edit distance with traceback\n",
    "    dp = np.zeros((real_len + 1, trans_len + 1), dtype=int)\n",
    "    \n",
    "    # Initialize\n",
    "    for i in range(real_len + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(trans_len + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Fill DP table\n",
    "    for i in range(1, real_len + 1):\n",
    "        for j in range(1, trans_len + 1):\n",
    "            if real_word_lower[i-1] == transcribed_word_lower[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j],      # deletion\n",
    "                                   dp[i][j-1],      # insertion  \n",
    "                                   dp[i-1][j-1])    # substitution\n",
    "    \n",
    "    # Traceback to find alignment\n",
    "    i, j = real_len, trans_len\n",
    "    real_aligned = []\n",
    "    trans_aligned = []\n",
    "    \n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and real_word_lower[i-1] == transcribed_word_lower[j-1]:\n",
    "            real_aligned.append(real_word_lower[i-1])\n",
    "            trans_aligned.append(transcribed_word_lower[j-1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and j > 0 and dp[i][j] == dp[i-1][j-1] + 1:\n",
    "            real_aligned.append(real_word_lower[i-1])\n",
    "            trans_aligned.append(transcribed_word_lower[j-1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and dp[i][j] == dp[i-1][j] + 1:\n",
    "            real_aligned.append(real_word_lower[i-1])\n",
    "            trans_aligned.append('-')\n",
    "            i -= 1\n",
    "        else:\n",
    "            real_aligned.append('-')\n",
    "            trans_aligned.append(transcribed_word_lower[j-1])\n",
    "            j -= 1\n",
    "    \n",
    "    real_aligned.reverse()\n",
    "    trans_aligned.reverse()\n",
    "    \n",
    "    # Now create the correctness array based on alignment\n",
    "    for idx, char in enumerate(real_word):\n",
    "        if char in string.punctuation:\n",
    "            is_letter_correct.append(1)  # Punctuation is always correct\n",
    "        elif idx < len(real_aligned):\n",
    "            if real_aligned[idx] != '-' and real_aligned[idx] == trans_aligned[idx]:\n",
    "                is_letter_correct.append(1)\n",
    "            else:\n",
    "                is_letter_correct.append(0)\n",
    "        else:\n",
    "            is_letter_correct.append(0)\n",
    "    \n",
    "    return is_letter_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68459a02",
   "metadata": {},
   "source": [
    "## Flask Web App Overview\n",
    "\n",
    "This section sets up a Flask web application that provides a pronunciation training service for English and French. The main components are:\n",
    "\n",
    "- **App Initialization:**  \n",
    "\n",
    "- **Flask Routes:**  \n",
    "    - `/`: Serves the main HTML page.\n",
    "    - `/get_sample`: Returns a random sentence for practice in the requested language.\n",
    "    - `/score_pronunciation`: Accepts audio and text, processes the audio, scores pronunciation accuracy, and returns the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FLASK WEB APP ---\n",
    "app = Flask(__name__, template_folder='.')\n",
    "CORS(app)\n",
    "\n",
    "# --- Model Initialization ---\n",
    "print(\"Initializing models...\")\n",
    "asr_model_en = WhisperASRModel(model_name='openai/whisper-base')\n",
    "asr_model_fr = WhisperASRModel(model_name='openai/whisper-base')  # Same model for French\n",
    "phonem_converter_en = EngPhonemConverter()\n",
    "phonem_converter_fr = EpitranPhonemConverter(epitran.Epitran('fra-Latn'))  # French IPA converter\n",
    "\n",
    "trainers = {\n",
    "    'en': PronunciationTrainer(asr_model_en, phonem_converter_en),\n",
    "    'fr': PronunciationTrainer(asr_model_fr, phonem_converter_fr),  # Added French support\n",
    "}\n",
    "\n",
    "# Create sample databases for both languages\n",
    "database_folder = './databases'\n",
    "if not os.path.exists(database_folder):\n",
    "    os.makedirs(database_folder)\n",
    "\n",
    "# Create CSV files if they don't exist\n",
    "en_csv_path = os.path.join(database_folder, 'data_en.csv')\n",
    "fr_csv_path = os.path.join(database_folder, 'data_fr.csv')\n",
    "\n",
    "text_datasets = {\n",
    "    'en': TextDataset(en_csv_path),\n",
    "    'fr': TextDataset(fr_csv_path),  # Added French dataset\n",
    "}\n",
    "print(\"Initialization complete.\")\n",
    "\n",
    "# --- Flask Routes ---\n",
    "@app.route('/')\n",
    "def main():\n",
    "    \"\"\"Serves the main HTML page.\"\"\"\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/get_sample', methods=['POST'])\n",
    "def get_sample():\n",
    "    \"\"\"Provides a random sentence for practice in the specified language.\"\"\"\n",
    "    data = request.get_json()\n",
    "    language = data.get('language', 'en')\n",
    "    \n",
    "    if language in text_datasets:\n",
    "        sentence = text_datasets[language].get_random_sample()\n",
    "        return jsonify({'sentence': sentence})\n",
    "    return jsonify({'error': 'Language not supported'}), 400\n",
    "\n",
    "@app.route('/score_pronunciation', methods=['POST'])\n",
    "def score_pronunciation():\n",
    "    \"\"\"Main endpoint for scoring pronunciation in English or French.\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        real_text = data['text']\n",
    "        base64_audio = data['audio']\n",
    "        language = data.get('language', 'en')\n",
    "\n",
    "        if language not in trainers:\n",
    "            return jsonify({'error': 'Language not supported'}), 400\n",
    "\n",
    "        # Decode audio\n",
    "        file_bytes = base64.b64decode(base64_audio.split(',')[1])\n",
    "\n",
    "        # Save to temp directory\n",
    "        temp_dir = './temp'\n",
    "        if not os.path.exists(temp_dir):\n",
    "            os.makedirs(temp_dir)\n",
    "\n",
    "        temp_filename = os.path.join(temp_dir, f\"audio_{generate_random_string(12)}.wav\")\n",
    "        with open(temp_filename, 'wb') as f:\n",
    "            f.write(file_bytes)\n",
    "            \n",
    "        try:\n",
    "            # Load and process audio\n",
    "            signal, sr_native = sf.read(temp_filename)\n",
    "            if len(signal.shape) > 1:\n",
    "                signal = np.mean(signal, axis=1)\n",
    "            signal = signal.flatten().astype(np.float32)\n",
    "            \n",
    "            # Pad/trim audio\n",
    "            min_length = 16000\n",
    "            max_length = 480000\n",
    "            if signal.shape[0] < min_length:\n",
    "                signal = np.pad(signal, (0, min_length - signal.shape[0]))\n",
    "            elif signal.shape[0] > max_length:\n",
    "                signal = signal[:max_length]\n",
    "                \n",
    "        finally:\n",
    "            if os.path.exists(temp_filename):\n",
    "                os.remove(temp_filename)\n",
    "\n",
    "        # Resample to 16kHz\n",
    "        if sr_native != 16000:\n",
    "            resampler = Resample(orig_freq=sr_native, new_freq=16000)\n",
    "            signal = resampler(torch.tensor(signal, dtype=torch.float32)).numpy()\n",
    "\n",
    "        # Process with trainer for the specified language\n",
    "        trainer = trainers[language]\n",
    "        result = trainer.process_audio_for_text(signal, real_text)\n",
    "\n",
    "        return jsonify(result)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during pronunciation scoring: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return jsonify({'error': 'Failed to process audio', 'details': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=3000, debug=False, threaded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18b1e89",
   "metadata": {},
   "source": [
    "# Advanced AI Pronunciation Scorer – Complete Data Flow Documentation\n",
    "\n",
    "## System Execution Flow for Pronunciation Scoring Request\n",
    "\n",
    "When a user submits audio for the sentence *\"I ran as fast as I could to beat my friend.\"*, here's the complete step-by-step data flow through the system:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **System Initialization Phase** (One-time Setup)\n",
    "\n",
    "#### **Library Imports and Dependencies**\n",
    "The system begins by importing all necessary libraries:\n",
    "- **Standard libraries**: `json`, `base64`, `os`, `abc` for basic functionality\n",
    "- **Audio processing**: `soundfile`, `torch`, `torchaudio` for audio manipulation\n",
    "- **ML libraries**: `transformers` (Whisper), `numpy`, `pandas` for data processing\n",
    "- **Web framework**: `Flask`, `CORS` for web application\n",
    "- **Specialized tools**: `dtwalign` (DTW alignment), `epitran`, `eng_to_ipa` (phoneme conversion)\n",
    "\n",
    "#### **Flask Application Setup**\n",
    "```python\n",
    "app = Flask(__name__, template_folder='.')\n",
    "CORS(app)\n",
    "```\n",
    "Creates the web server with cross-origin support for frontend communication.\n",
    "\n",
    "#### **Dataset Loading via TextDataset Class**\n",
    "```python\n",
    "class TextDataset:\n",
    "  def __init__(self, filepath):\n",
    "    self.table_dataframe = pd.read_csv(filepath, delimiter=';')\n",
    "```\n",
    "**Purpose**: Manages practice sentences from CSV files for both English and French.\n",
    "**Why needed**: Provides a repository of sentences for users to practice pronunciation.\n",
    "\n",
    "#### **Model Interface Definitions**\n",
    "Two abstract base classes define contracts for the system:\n",
    "- **`IASRModel`**: Defines methods `getTranscript()`, `getWordLocations()`, `processAudio()`\n",
    "- **`ITextToPhonemModel`**: Defines method `convertToPhonem()`\n",
    "\n",
    "**Why needed**: Enables dependency injection and makes the system extensible for different ASR models and phoneme converters.\n",
    "\n",
    "#### **Concrete Model Implementations**\n",
    "\n",
    "**WhisperASRModel** (Speech Recognition):\n",
    "```python\n",
    "def __init__(self, model_name='openai/whisper-base'):\n",
    "  self.asr = pipeline('automatic-speech-recognition', model=model_name)\n",
    "```\n",
    "**Purpose**: Converts speech audio to text using OpenAI's Whisper model.\n",
    "**Why needed**: Core component that transcribes user's spoken words for comparison.\n",
    "\n",
    "**Phoneme Converters**:\n",
    "- **`EngPhonemConverter`**: Uses `eng_to_ipa` library for English phonetic conversion\n",
    "- **`EpitranPhonemConverter`**: Uses `epitran` library for French phonetic conversion\n",
    "\n",
    "**Why needed**: Enable accurate pronunciation comparison by converting text to International Phonetic Alphabet (IPA) representations.\n",
    "\n",
    "#### **PronunciationTrainer Setup**\n",
    "```python\n",
    "trainers = {\n",
    "  'en': PronunciationTrainer(asr_model_en, phonem_converter_en),\n",
    "  'fr': PronunciationTrainer(asr_model_fr, phonem_converter_fr)\n",
    "}\n",
    "```\n",
    "**Purpose**: Orchestrates the entire pronunciation analysis pipeline.\n",
    "**Why needed**: Central coordinator that combines ASR and phoneme conversion for complete analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Request Processing Pipeline**\n",
    "\n",
    "When a user makes a POST request to `/score_pronunciation` with:\n",
    "```json\n",
    "{\n",
    "  \"text\": \"I ran as fast as I could to beat my friend.\",\n",
    "  \"audio\": \"data:audio/wav;base64,UklGRnoGAABXQVZFZm10...\",\n",
    "  \"language\": \"en\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Step 1: Request Reception and Validation**\n",
    "```python\n",
    "@app.route('/score_pronunciation', methods=['POST'])\n",
    "def score_pronunciation():\n",
    "  data = request.get_json()\n",
    "  real_text = data['text']\n",
    "  base64_audio = data['audio']\n",
    "  language = data.get('language', 'en')\n",
    "```\n",
    "**Data Flow**: JSON payload → Extract text, audio, language → Validate language support\n",
    "\n",
    "#### **Step 2: Audio Decoding and Preprocessing**\n",
    "```python\n",
    "file_bytes = base64.b64decode(base64_audio.split(',')[1])\n",
    "signal, sr_native = sf.read(temp_filename)\n",
    "```\n",
    "**Process**:\n",
    "1. Decode Base64 audio to raw bytes\n",
    "2. Save to temporary WAV file\n",
    "3. Load using `soundfile` library\n",
    "4. Convert stereo to mono if necessary\n",
    "5. Normalize to float32 format\n",
    "6. Pad/trim to acceptable length (1-30 seconds)\n",
    "7. Resample to 16kHz using `torchaudio.Resample`\n",
    "\n",
    "**Why needed**: Whisper requires specific audio format (16kHz, mono, float32) for optimal performance.\n",
    "\n",
    "#### **Step 3: Trainer Selection and Audio Processing**\n",
    "```python\n",
    "trainer = trainers[language]\n",
    "result = trainer.process_audio_for_text(signal, real_text)\n",
    "```\n",
    "**Data Flow**: Audio signal + reference text → Language-specific trainer → Complete analysis\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **PronunciationTrainer Processing Pipeline**\n",
    "\n",
    "#### **Step 3a: Audio Normalization**\n",
    "```python\n",
    "def _preprocess_audio(self, audio: np.ndarray) -> np.ndarray:\n",
    "  audio = audio - np.mean(audio)  # Zero mean\n",
    "  audio = audio / np.max(np.abs(audio))  # Scale to [-1,1]\n",
    "```\n",
    "**Purpose**: Standardizes audio amplitude for consistent ASR performance.\n",
    "\n",
    "#### **Step 3b: Speech Recognition**\n",
    "```python\n",
    "def _get_transcript_and_locations(self, audio: np.ndarray):\n",
    "  self.asr_model.processAudio(audio)\n",
    "  transcript = self.asr_model.getTranscript()\n",
    "```\n",
    "**Data Flow**: \n",
    "- Normalized audio → WhisperASRModel → Transcribed text\n",
    "- Example: Audio → *\"I ran as fast as I could to beat my friend\"*\n",
    "\n",
    "#### **Step 3c: Word Alignment using Dynamic Time Warping**\n",
    "```python\n",
    "words_real = real_text.split()  # ['I', 'ran', 'as', 'fast', 'as', 'I', 'could', 'to', 'beat', 'my', 'friend.']\n",
    "words_estimated = recording_transcript.split()  # ['I', 'ran', 'as', 'fast', 'as', 'I', 'could', 'to', 'beat', 'my', 'friend']\n",
    "mapped_words, _ = get_best_mapped_words(words_estimated, words_real)\n",
    "```\n",
    "\n",
    "**Word Alignment Process**:\n",
    "1. **`get_word_distance_matrix()`**: Creates cost matrix using edit distance with bonuses for:\n",
    "   - Words starting with same letters\n",
    "   - Similar length words\n",
    "2. **DTW Alignment**: Uses `dtw_from_distance_matrix()` to find optimal word pairing\n",
    "3. **Post-processing**: Finds good matches DTW might have missed\n",
    "\n",
    "**Why needed**: User speech may have timing variations, missing words, or pronunciation errors that need intelligent alignment.\n",
    "\n",
    "#### **Step 3d: Pronunciation Accuracy Calculation**\n",
    "```python\n",
    "def get_pronunciation_accuracy(self, real_and_transcribed_words: list) -> tuple:\n",
    "```\n",
    "**Process for each word pair**:\n",
    "1. Clean punctuation from both words\n",
    "2. Convert to IPA using phoneme converter:\n",
    "   - *\"ran\"* → *\"ɹæn\"*\n",
    "   - *\"fast\"* → *\"fæst\"*\n",
    "3. Calculate edit distance between IPA representations\n",
    "4. Compute word-level accuracy: `((phonemes - distance) / phonemes) * 100`\n",
    "5. Apply context-aware scoring:\n",
    "   - If >60% of words transcribed correctly, boost overall score\n",
    "   - Give partial credit to missing words in good contexts\n",
    "\n",
    "**Why needed**: Raw word matching isn't sufficient; phonetic similarity provides more accurate pronunciation assessment.\n",
    "\n",
    "#### **Step 3e: Categorization and Letter-Level Analysis**\n",
    "```python\n",
    "def get_words_pronunciation_category(self, accuracies: list) -> list:\n",
    "  # Convert scores to categories: 1=Good (80%+), 2=Medium (50-80%), 3=Poor (<50%)\n",
    "```\n",
    "\n",
    "```python\n",
    "def get_which_letters_were_correct(real_word: str, transcribed_word: str) -> list:\n",
    "```\n",
    "**Process**:\n",
    "1. Use dynamic programming for character-level alignment\n",
    "2. Generate binary correctness array for each character\n",
    "3. Handle punctuation (always marked correct)\n",
    "\n",
    "**Why needed**: Provides granular feedback for frontend highlighting of correct/incorrect letters.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Response Assembly and Return**\n",
    "\n",
    "#### **Final Result Compilation**\n",
    "```python\n",
    "result = {\n",
    "  'real_transcript': real_text,\n",
    "  'recording_transcript': recording_transcript,\n",
    "  'matched_transcripts': ' '.join(mapped_words),\n",
    "  'pronunciation_accuracy': str(int(pronunciation_accuracy)),\n",
    "  'pair_accuracy_category': ' '.join(map(str, pronunciation_categories)),\n",
    "  'is_letter_correct_all_words': is_letter_correct_all_words.strip()\n",
    "}\n",
    "```\n",
    "\n",
    "#### **Sample Output for \"I ran as fast as I could to beat my friend.\"**\n",
    "```json\n",
    "{\n",
    "  \"real_transcript\": \"I ran as fast as I could to beat my friend.\",\n",
    "  \"recording_transcript\": \"I ran as fast as I could to beat my friend\",\n",
    "  \"matched_transcripts\": \"I ran as fast as I could to beat my friend -\",\n",
    "  \"pronunciation_accuracy\": \"96\",\n",
    "  \"pair_accuracy_category\": \"1 1 1 1 1 1 1 1 1 1 3\",\n",
    "  \"is_letter_correct_all_words\": \"1 111 11 1111 11 1 11111 11 1111 11 111111\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Key Algorithms and Their Purposes**\n",
    "\n",
    "#### **Edit Distance (Levenshtein)**\n",
    "- **Used in**: Word alignment, phoneme comparison, character alignment\n",
    "- **Purpose**: Measures similarity between sequences\n",
    "- **Why critical**: Handles variations in pronunciation and transcription\n",
    "\n",
    "#### **Dynamic Time Warping (DTW)**\n",
    "- **Used in**: Word sequence alignment\n",
    "- **Purpose**: Finds optimal alignment between reference and transcribed words\n",
    "- **Why needed**: Speech timing varies; DTW handles insertions, deletions, and substitutions\n",
    "\n",
    "#### **Context-Aware Scoring**\n",
    "- **Purpose**: Prevents over-penalization when most words are correct\n",
    "- **Logic**: If >60% words transcribed, boost scores and give partial credit to missing words\n",
    "- **Why important**: Provides encouraging, realistic feedback for language learners\n",
    "\n",
    "---\n",
    "\n",
    "This comprehensive pipeline transforms raw audio input into detailed pronunciation feedback, enabling effective language learning through precise phonetic analysis and user-friendly visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
