{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22156938",
   "metadata": {},
   "source": [
    "I had the English dataset which had thousands of sentences, and needed a corresponding French translation. Before I decided to remove a lot of the sentences as they're not needed, I thought I had to translate the majority of, a service for which I couldn't find for free.\n",
    "\n",
    "A custom solution where I hardcode the corresponding Helsink NLP and desired language led to a nice tool that can translate the entire english dataset to my desired language, in this case French. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585b0de",
   "metadata": {},
   "source": [
    "## English to French Dataset Translation Tool\n",
    "\n",
    "This code implements an automated translation system that converts English sentence datasets to French using machine learning models. The solution addresses the challenge of creating multilingual datasets without relying on expensive translation services.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "**Translation Pipeline:**\n",
    "- Uses Helsinki-NLP's `opus-mt-en-fr` model for high-quality English-to-French translation\n",
    "- Processes sentences from `./databases/data_en.csv` (limited to first 100 rows for efficiency)\n",
    "- Implements robust error handling with fallback mechanisms\n",
    "\n",
    "**Core Functions:**\n",
    "- `create_french_from_english_csv()`: Main translation function that reads English sentences, translates them using the Helsinki-NLP model, and saves the results\n",
    "- `verify_french_dataset()`: Validation function that checks the output file integrity and reports statistics\n",
    "\n",
    "**Features:**\n",
    "- **Batch Processing**: Translates entire datasets sentence by sentence\n",
    "- **Progress Tracking**: Real-time display of translation progress with source and target text previews\n",
    "- **Error Recovery**: Keeps original sentences when translation fails\n",
    "- **Data Validation**: Checks for empty sentences and provides dataset statistics\n",
    "- **UTF-8 Support**: Ensures proper encoding for French characters\n",
    "\n",
    "### Technical Implementation\n",
    "\n",
    "The tool leverages the Transformers library with PyTorch backend, using beam search decoding for optimal translation quality. Output is saved as a semicolon-delimited CSV file compatible with the existing data structure.\n",
    "\n",
    "This custom solution eliminates the need for paid translation APIs while maintaining translation quality suitable for pronunciation training applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab3931",
   "metadata": {},
   "source": [
    "![addLanguage_end](databases/addLanguage_start.png)\n",
    "\n",
    "![addLanguage_end](databases/addLanguage_end.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38390aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "def create_french_from_english_csv():\n",
    "    \"\"\"\n",
    "    Creates a French dataset by translating sentences from the existing English CSV file.\n",
    "    \"\"\"\n",
    "    # Read the existing English dataset\n",
    "    en_csv_path = './databases/data_en.csv'\n",
    "    \n",
    "    if not os.path.exists(en_csv_path):\n",
    "        print(f\"Error: English dataset not found at {en_csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Read the English CSV file\n",
    "        df_english = pd.read_csv(en_csv_path, delimiter=';')\n",
    "        # only first 100 rows\n",
    "        df_english = df_english.head(100)\n",
    "        print(f\"Loaded English dataset with {len(df_english)} sentences from {en_csv_path}\")\n",
    "        \n",
    "        # Extract sentences from the dataframe\n",
    "        english_sentences = df_english['sentence'].tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading English CSV file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Initializing Helsinki-NLP English to French translation model...\")\n",
    "    \n",
    "    # Initialize Helsinki-NLP English to French translation model\n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        print(\"Model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading translation model: {e}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Translating {len(english_sentences)} English sentences to French...\")\n",
    "    \n",
    "    french_sentences = []\n",
    "    \n",
    "    for i, sentence in enumerate(english_sentences):\n",
    "        try:\n",
    "            # Skip empty or NaN sentences\n",
    "            if pd.isna(sentence) or not sentence.strip():\n",
    "                print(f\"  {i+1:3d}. Skipping empty sentence\")\n",
    "                continue\n",
    "                \n",
    "            # Tokenize the input sentence\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            \n",
    "            # Generate translation\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=128,\n",
    "                    num_beams=4,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False\n",
    "                )\n",
    "            \n",
    "            # Decode the translation\n",
    "            translated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            french_sentences.append(translated)\n",
    "            \n",
    "            print(f\"  {i+1:3d}. EN: {sentence[:50]}{'...' if len(sentence) > 50 else ''}\")\n",
    "            print(f\"       FR: {translated[:50]}{'...' if len(translated) > 50 else ''}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error translating sentence {i+1}: {e}\")\n",
    "            # Keep the original sentence as fallback\n",
    "            french_sentences.append(sentence)\n",
    "            print(f\"  {i+1:3d}. Translation failed, keeping original: {sentence[:50]}{'...' if len(sentence) > 50 else ''}\")\n",
    "    \n",
    "    # Create databases directory if it doesn't exist\n",
    "    database_folder = './'\n",
    "    if not os.path.exists(database_folder):\n",
    "        os.makedirs(database_folder)\n",
    "        print(f\"Created directory: {database_folder}\")\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df_french = pd.DataFrame({'sentence': french_sentences})\n",
    "    fr_csv_path = os.path.join(database_folder, 'data_fr.csv')\n",
    "    df_french.to_csv(fr_csv_path, sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\nFrench dataset created successfully!\")\n",
    "    print(f\"Source file: {en_csv_path}\")\n",
    "    print(f\"Output file: {fr_csv_path}\")\n",
    "    print(f\"Number of sentences translated: {len(french_sentences)}\")\n",
    "    \n",
    "    # Display the first few sentences for verification\n",
    "    print(\"\\nFirst 5 French sentences:\")\n",
    "    for i, sentence in enumerate(french_sentences[:5]):\n",
    "        print(f\"  {i+1}. {sentence}\")\n",
    "    \n",
    "    return fr_csv_path\n",
    "\n",
    "def verify_french_dataset():\n",
    "    \"\"\"\n",
    "    Verifies that the French dataset was created correctly.\n",
    "    \"\"\"\n",
    "    fr_csv_path = './databases/data_fr.csv'\n",
    "    \n",
    "    if not os.path.exists(fr_csv_path):\n",
    "        print(\"French dataset not found!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(fr_csv_path, delimiter=';')\n",
    "        print(f\"\\nDataset verification:\")\n",
    "        print(f\"  File: {fr_csv_path}\")\n",
    "        print(f\"  Number of sentences: {len(df)}\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Check for any missing or empty sentences\n",
    "        empty_sentences = df['sentence'].isna().sum()\n",
    "        if empty_sentences > 0:\n",
    "            print(f\"  Warning: {empty_sentences} empty sentences found!\")\n",
    "        else:\n",
    "            print(\"  All sentences are valid!\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying dataset: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== French Dataset Creator from English CSV ===\")\n",
    "    print(\"This script reads from databases/data_en.csv and creates data_fr.csv\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Create French dataset from English CSV\n",
    "        csv_path = create_french_from_english_csv()\n",
    "        \n",
    "        if csv_path:\n",
    "            # Verify the dataset\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            verify_french_dataset()\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"French dataset creation completed!\")\n",
    "            print(\"You can now use French in your pronunciation trainer.\")\n",
    "            print(\"Make sure to install epitran for French IPA conversion:\")\n",
    "            print(\"  pip install epitran\")\n",
    "        else:\n",
    "            print(\"Failed to create French dataset.\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nScript interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(\"Failed to create French dataset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
